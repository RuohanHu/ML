{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import operator\n",
    "\n",
    "class Solver():\n",
    "    def __init__(self):\n",
    "        # list that contains all the training data\n",
    "        self.training_data = []\n",
    "        # list that contains all the test data\n",
    "        self.test_data = []\n",
    "    def load_data(self):\n",
    "        # open the training data file\n",
    "        training_file = open(\"pa4train.txt\")\n",
    "        # load the training data\n",
    "        for line in training_file:\n",
    "            data = line.split()\n",
    "            data[-1] = int(data[-1])\n",
    "            self.training_data.append(data)\n",
    "        # close the training data file\n",
    "        training_file.close()\n",
    "        # open the test data file \n",
    "        test_file = open(\"pa4test.txt\")\n",
    "        # load the test data\n",
    "        for line in test_file:\n",
    "            data = line.split()\n",
    "            data[-1] = int(data[-1])\n",
    "            self.test_data.append(data)\n",
    "        # close the test data file\n",
    "        test_file.close()\n",
    "    def kernel1(self, s, t, p):\n",
    "        # dictionary that maps the substring to its appearances in s\n",
    "        map1 = {}\n",
    "        # get all the substrings of length p in string s\n",
    "        for i in range(0, (len(s) - p + 1)):\n",
    "            # get the substring\n",
    "            substring = s[i : (i + p)]\n",
    "            # update the dictionary\n",
    "            if substring not in map1:\n",
    "                map1[substring] = 1\n",
    "            else:\n",
    "                map1[substring] = map1[substring] + 1\n",
    "        # dictionary that maps the substring to its appearances in t\n",
    "        map2 = {}\n",
    "        # get all the substrings of length p in string t\n",
    "        for i in range(0, (len(t) - p + 1)):\n",
    "            # get the substring\n",
    "            substring = t[i : (i + p)]\n",
    "            # update the dictionary\n",
    "            if substring not in map2:\n",
    "                map2[substring] = 1\n",
    "            else:\n",
    "                map2[substring] = map2[substring] + 1\n",
    "        # initialize the count\n",
    "        count = 0\n",
    "        for i in map1:\n",
    "            if i in map2:\n",
    "                # update the count\n",
    "                count = count + map1[i] * map2[i]\n",
    "        return count\n",
    "    def kernel2(self, s, t, p):\n",
    "        # dictionary that maps the substring to its appearances in s\n",
    "        map1 = {}\n",
    "        # get all the substrings of length p in string s\n",
    "        for i in range(0, (len(s) - p + 1)):\n",
    "            # get the substring\n",
    "            substring = s[i : (i + p)]\n",
    "            # update the dictionary\n",
    "            if substring not in map1:\n",
    "                map1[substring] = 1\n",
    "        # dictionary that maps the substring to its appearances in t\n",
    "        map2 = {}\n",
    "        # get all the substrings of length p in string t\n",
    "        for i in range(0, (len(t) - p + 1)):\n",
    "            # get the substring\n",
    "            substring = t[i : (i + p)]\n",
    "            # update the dictionary\n",
    "            if substring not in map2:\n",
    "                map2[substring] = 1\n",
    "        # initialize the count\n",
    "        count = 0\n",
    "        for i in map1:\n",
    "            if i in map2:\n",
    "                # update the count\n",
    "                count = count + map1[i] * map2[i]\n",
    "        return count\n",
    "    def kernelization(self, option, p, current_data, data_set):\n",
    "        # return 0 if the data set is empty\n",
    "        if len(data_set) == 0:\n",
    "            return 0\n",
    "        # initialize the value\n",
    "        value = 0\n",
    "        # calculate the value\n",
    "        for i in data_set:\n",
    "            if option == 1:\n",
    "                value = value + i[-1] * self.kernel1(current_data[0], i[0], p)\n",
    "            else:\n",
    "                value = value + i[-1] * self.kernel2(current_data[0], i[0], p)\n",
    "        return value\n",
    "    def perceptron(self, option, p):\n",
    "        # list that contains all the training data which requires update of the classifier\n",
    "        data_set = []\n",
    "        # iterate through the training data\n",
    "        for i in self.training_data:\n",
    "            if (i[-1] * self.kernelization(option, p, i, data_set)) <= 0:\n",
    "                data_set.append(i)\n",
    "        # return the data set\n",
    "        return data_set\n",
    "    def calculate_errors(self, option, p, classifier):\n",
    "        # initialize the training error\n",
    "        training_error = 0.0\n",
    "        errors = 0\n",
    "        # iterate through the training data\n",
    "        for i in self.training_data:\n",
    "            # make the classification\n",
    "            classification = numpy.sign(self.kernelization(option, p, i, classifier))\n",
    "            if classification == 0:\n",
    "                classification = -1\n",
    "            # check whether the classification is correct\n",
    "            if classification != i[-1]:\n",
    "                errors = errors + 1\n",
    "        # calculate the training error\n",
    "        training_error = float(errors) / float(len(self.training_data))\n",
    "        # initialize the test error\n",
    "        test_error = 0.0\n",
    "        errors = 0\n",
    "        # iterate through the training data\n",
    "        for i in self.test_data:\n",
    "            # make the classification\n",
    "            classification = numpy.sign(self.kernelization(option, p, i, classifier))\n",
    "            if classification == 0:\n",
    "                classification = -1\n",
    "            # check whether the classification is correct\n",
    "            if classification != i[-1]:\n",
    "                errors = errors + 1\n",
    "        # calculate the test error\n",
    "        test_error = float(errors) / float(len(self.test_data))\n",
    "        return training_error, test_error\n",
    "    def find_coordinates(self, classifier):\n",
    "        feature_map = {}\n",
    "        # iterate through the training data\n",
    "        for i in classifier:\n",
    "            # get all the substrings of length 5\n",
    "            for j in range(0, (len(i[0]) - 5 + 1)):\n",
    "                substring = i[0][j : (j + 5)]\n",
    "                # update the feature map\n",
    "                if substring not in feature_map:\n",
    "                    feature_map[substring] = i[-1]\n",
    "                else:\n",
    "                    feature_map[substring] = feature_map[substring] + i[-1]\n",
    "        # sort the feature\n",
    "        sorted_map = sorted(feature_map.items(), key=operator.itemgetter(1))\n",
    "        print(sorted_map)\n",
    "        # find the substrings\n",
    "        print(sorted_map[-1])\n",
    "        print(sorted_map[-2])\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # create the solver\n",
    "    solver = Solver()\n",
    "    # load the data\n",
    "    solver.load_data()\n",
    "    \n",
    "    print(\"String Kernel K\")\n",
    "    print(\"p = 2\")\n",
    "    data_set = solver.perceptron(1, 2)\n",
    "    # calculate the errors\n",
    "    training_error, test_error = solver.calculate_errors(1, 2, data_set)\n",
    "    print(\"training error:\", training_error, \"test error:\", test_error)\n",
    "    print(\"p = 3\")\n",
    "    data_set = solver.perceptron(1, 3)\n",
    "    # calculate the errors\n",
    "    training_error, test_error = solver.calculate_errors(1, 3, data_set)\n",
    "    print(\"training error:\", training_error, \"test error:\", test_error)\n",
    "    print(\"p = 4\")\n",
    "    data_set = solver.perceptron(1, 4)\n",
    "    # calculate the errors\n",
    "    training_error, test_error = solver.calculate_errors(1, 4, data_set)\n",
    "    print(\"training error:\", training_error, \"test error:\", test_error)\n",
    "    print(\"p = 5\")\n",
    "    data_set = solver.perceptron(1, 5)\n",
    "    # find the two coordinates\n",
    "    solver.find_coordinates(data_set)\n",
    "    # calculate the errors\n",
    "    training_error, test_error = solver.calculate_errors(1, 5, data_set)\n",
    "    print(\"training error:\", training_error, \"test error:\", test_error)\n",
    "\n",
    "    print(\"String Kernel M\")\n",
    "    print(\"p = 3\")\n",
    "    data_set = solver.perceptron(2, 3)\n",
    "    # calculate the errors\n",
    "    training_error, test_error = solver.calculate_errors(2, 3, data_set)\n",
    "    print(\"training error:\", training_error, \"test error:\", test_error)\n",
    "    print(\"p = 4\")\n",
    "    data_set = solver.perceptron(2, 4)\n",
    "    # calculate the errors\n",
    "    training_error, test_error = solver.calculate_errors(2, 4, data_set)\n",
    "    print(\"training error:\", training_error, \"test error:\", test_error)\n",
    "    print(\"p = 5\")\n",
    "    data_set = solver.perceptron(2, 5)\n",
    "    # calculate the errors\n",
    "    training_error, test_error = solver.calculate_errors(2, 5, data_set)\n",
    "    print(\"training error:\", training_error, \"test error:\", test_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
